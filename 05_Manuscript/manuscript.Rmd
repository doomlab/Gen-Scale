---
title             : "The title"
shorttitle        : "Title"
author: 
  - name          : "First Author"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Postal address"
    email         : "my@email.com"
    role: # Contributorship roles (e.g., CRediT, https://credit.niso.org/)
      - "Conceptualization"
      - "Writing - Original Draft Preparation"
      - "Writing - Review & Editing"
  - name          : "Ernst-August Doelle"
    affiliation   : "1,2"
    role:
      - "Writing - Review & Editing"
      - "Supervision"
affiliation:
  - id            : "1"
    institution   : "Wilhelm-Wundt-University"
  - id            : "2"
    institution   : "Konstanz Business School"
authornote: |
  Add complete departmental affiliations for each author here. Each new line herein must be indented, like this line.
  Enter author note here.
abstract: |
  One or two sentences providing a **basic introduction** to the field,  comprehensible to a scientist in any discipline.
  Two to three sentences of **more detailed background**, comprehensible  to scientists in related disciplines.
  One sentence clearly stating the **general problem** being addressed by  this particular study.
  One sentence summarizing the main result (with the words "**here we show**" or their equivalent).
  Two or three sentences explaining what the **main result** reveals in direct comparison to what was thought to be the case previously, or how the  main result adds to previous knowledge.
  One or two sentences to put the results into a more **general context**.
  Two or three sentences to provide a **broader perspective**, readily comprehensible to a scientist in any discipline.
  
  
  <!-- https://tinyurl.com/ybremelq -->
keywords          : "keywords"
wordcount         : "X"
floatsintext      : no
linenumbers       : yes
draft             : no
mask              : no
figurelist        : no
tablelist         : no
footnotelist      : no
classoption       : "man"
output            : papaja::apa6_pdf
bibliography: references.bib
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include = FALSE}
library("papaja")
r_refs("r-references.bib")
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

```{r trackdown, eval = F}
library(trackdown)
source("~/GitHub/files/google_creds.R")
# upload_file(
#   file = "manuscript.Rmd",
#   gfile = "gen_scale_manuscript_trackdown",
#   gpath = "gen_scale/",
#   hide_code = TRUE
# )

# pull to me 
download_file(
  file = "manuscript.Rmd",
  gfile = "gen_scale_manuscript_trackdown",
  gpath = "gen_scale/",
  hide_code = TRUE
)

# push to google
update_file(
  file = "manuscript.Rmd",
  gfile = "gen_scale_manuscript_trackdown",
  gpath = "gen_scale/",
  hide_code = TRUE
)
```

Death is something each of us must learn to cope with, whether in
healthy ways or less so. These issues may be at front of mind for many
in light of the COVID-19 pandemic. Various existential philosophers and
psychologists have proposed ways in which we deal with the awareness of
death and the anxiety this awareness often causes. Psychoanalyst Erik
Erikson [-@erikson1950] proposed that during mid-life one becomes
acutely aware of their oncoming death and is motivated to care for
things which will outlast themselves. He called this act of caring
generativity. In *The Denial of Death*, philosopher Ernest Becker
[-@becker1973] posits that humans undertake immortality projects to curb
their sense of vulnerability to death. Similarly, psychiatrist Robert
Jay Lifton [-@lifton1979], a mentee of Erikson, described the awareness
of death as being ever present and motivating us to create symbols,
thereby allowing us to imagine ourselves as symbolically immortalized.
Existential psychiatrist Irvin Yalom [-@yalom2008] notes that many of
his clients experiencing anxiety about their death take comfort in
“rippling,” the idea that one’s lasting effects on the world will ripple
out and influence the world after they have died.

Although these thinkers use different terminology, there are several
common themes among their ideas. (1) Our physical death is an
inevitability, and we often find our awareness of its inevitability to
be aversive. This aversion may be referred to variously as angst,
death-anxiety, despair, being-towards-death, terror, and so on. However,
(2) we take comfort in the idea that other, non-physical parts of us
continue to exist indefinitely after our biological death, through
mechanisms such as the heroic archetype and symbolic self. (3) Finally,
we can take action to promote these non-physical aspects of the self,
such as through search for meaning, sense of immortality, care,
generativity, and rippling.

One of these bodies of thought, called symbolic immortality, was
originally theorized by Lifton [-@lifton1979], who thought that
awareness of death drives a fundamental human desire for a sense of
continuity lasting beyond the lifespan. Essentially, humans are
meaning-seeking creatures, and throughout our lives, this search for
meaning involves an evolving psychological imagery of life and death.
Death, or the transient nature of life, threatens our search for
meaning. Lifton thought that if we could achieve what we believe to be
some form of immortality, we could overcome this loss of meaning, and
the awareness of death could instead drive an inner vitality (imagery
associated with connection, integrity, and movement). If this drive
toward vitality is lost, we are vulnerable to a psychic numbness or
death-in-life (imagery associated with separation, disintegration, and
stasis). In Lifton's own words, "Death does indeed bring about
biological and psychic annihilation. But life includes symbolic
perceptions of connections that precede and outlast that annihilation"
[-@lifton1979, p. 18].

Lifton [-@lifton1979] proposed five modes of experience or ways of
achieving symbolic immortality: The biological (or biosocial) mode in
which one lives on through their genetic and sociocultural progeny, the
creative mode in which one’s accomplishments and contribution outlast
oneself, the natural mode in which one feels they are a part of the
broader universe, the spiritual mode in which one seeks to transcend the
physical realm to a higher spiritual realm beyond death, and the mode of
experiential transcendence in which one experiences a phenomenological
state of flow. The experiential mode must occur in the context of at
least one of the other four to really be considered transcendent, but it
is thought to have a great capacity to bring about personal change.

Claims of how we suppress death-anxiety have been investigated
experimentally, primarily through the paradigm of Terror Management
Theory (TMT). Based on the theories of Ernest Becker, TMT posits that
human awareness of death is always present to some degree. This
awareness of our inevitable death, coupled with a strong aversion to
thoughts of death, causes terror and is pushed out of our consciousness
by our creation of meaning systems [@greenberg1986]. TMT proposes that
self-esteem, interpersonal relationships, and cultural worldview work
together to buffer against our anxiety about death. It is assumed that
these buffers suppress thoughts of death by providing a sense of
symbolic immortality, though little systematic research has been
conducted on this construct. The results of this buffering process are
not always positive. For example, experimentally priming mortality
salience can lead to more positive attitudes toward in-group members but
harsher negative attitudes toward out-group members [@greenberg1990].

TMT refers to a person's awareness of death as mortality salience (MS).
The MS hypothesis of TMT posits than an increase in one's awareness of
death causes an increase in compensatory behaviors to lower their death
anxiety, either by distracting from the awareness of death or by the
promotion of meaningful cultural worldviews. In the MS paradigm,
experimentally priming a participant’s awareness of death (for example,
by having participants write about death and then complete a distraction
task) is thought to cause an increase in compensatory buffers. A
meta-analysis of 277 experiments found mortality salience to have a
robust, moderate overall effect size: *r*(276) = 0.35, *p* = .00
[@burke2010]. Altogether, these experiments provide convincing evidence
for TMT and the MS hypothesis in particular.

Though some avoidance of (or buffering against) death anxiety is thought
to be universal and has the potential to increase interpersonal
conflict, awareness of death through symbolic immortality may also have
potential as a positive force. In particular, it is thought to be an
underlying motive for what Erikson referred to as generativity.
Generativity is the seventh of eight proposed stages in Erikson's
[-@erikson1950] theory of psychosocial development, which he associated
with midlife and described as "the concern in establishing and guiding
the next generation" [@erikson1963, p. 267]. Little systematic research
was conducted on this subject until the 1980's. Kotre [-@kotre1984]
expanded on the theory and proposed that the drive for generativity was
related to a motive to expand the sense of self beyond the lifetime,
especially in light of the fear of death.

McAdams and de St. Aubin [-@mcadams1992] sought to formalize the study
of generativity as a multidimensional construct. Their seven components
of generativity include cultural demand, inner desire (for symbolic
immortality and community), concern (for the next generation), belief
(in the human species), commitment, action, and narration (of
generativity within one's life story). In addition to a quantitative
measure of generative concern (the Loyola Generativity Scale), they
developed a system for content analysis of autobiographical episodes
pertaining to generativity, and symbolic immortality is one of the five
themes they found. Here they define symbolic immortality as "any
reference to leaving a legacy, having an enduring influence, or leaving
behind products that will outlive one's physical existence," a theme
clearly related to both Lifton's and Erikson's theories [@mcadams1992,
p. 1011].

These research areas depend on the construct of symbolic immortality for
their theoretical frameworks, but few researchers have attempted to
systematically and quantitatively assess this construct. Two attempts
have been made to develop such a measurement: Drolet's [-@drolet1990]
Sense of Symbolic Immortality Scale and Mathews and Kling's
[-@mathews1988] measure of symbolic immortality, based on an original
questionnaire by Mathews and Mister [-@mathews1987].

Drolet [-@drolet1990] developed the Sense of Symbolic Immortality Scale
based on Robert J. Lifton’s theory of symbolic immortality and its five
modes of experience. Drolet studied 136 adults, ages 18-30 and 30-40,
and hypothesized that those in their 30's (established adults) would
have a greater sense of symbolic immortality than the young adults
(18-30). The measure is inherently subjective, not only by the nature of
self-report, but in that the scale seeks to measure what a person
*believes* and how they *feel* about these subjects. The scale as a
whole had a high internal consistency ($\alpha$ = .91) and test-retest
reliability was *r* = .97. Internal consistency of subscales for the
five theoretical modes of immortality was mixed. Of the five, spiritual
immortality was the most distinct from the scale as a whole and the
other subscales. Factor analysis showed three factors, mapping onto
biosocial, creative, and spiritual. The transcendent and natural items
may be closely related to biosocial.

Moving beyond the scale development itself, SSI correlated negatively
with Templer's Death Anxiety Scale and had a strong (*r* = .84) positive
relationship with Maholick's Purpose in Life Test [@drolet1990]. In
interpreting the very strong correlation, the author suggests that SSI
is a broader construct than Purpose in Life and the scale itself may be
less prone to social desirability effects than the PIL, although this
had not been directly tested. Age group was also related, with
established adults having a higher SSI, particularly in the biosocial
and creative domains.

We see multiple issues with using the Symbolic Immortality Scale. First,
the study was underpowered, conducting exploratory factor analysis of 67
items using a sample of 136. Second, the scale was developed in French,
and we do not take for granted the psychometric properties of a
translated version. Third and most fundamentally, the scale has poor
face validity and appears to measure the constructs theorized to
symbolically immortalize rather than a sense of symbolic immortality
directly. For example, the scale includes items such as “My sex life
contributes greatly to my well-being”, “Intimate relationships scare
me”, and “I am sure of who I am." Although related to the constructs
(such as interpersonal relationships and self-esteem) which
theoretically help cope with death, it is unclear how these items
represent the construct of symbolic immortality itself.

Mathews and Mister [-@mathews1987] also developed a scale pertaining to
symbolic immortality, sensation seeking, and psychic numbness in a study
including 400 adults. Experiential transcendence was operationalized as
similar to Zuckerman's [-@zuckerman1979] sensation seeking, which may
not fully capture the original intent (the experience of losing
oneself). Items were mapped onto five factors, and the five factors
largely aligned with Lifton's constructs. Although internal consistency
was at least acceptable for each factor, goodness of fit statistics are
not reported. Some studies have used a revised version of the scale by
Mathews and Kling [-@mathews1988], who adapted it for a study on
prosocial behavior in the context of nonprofit volunteer motivation.
They reported similar results for their revised scale. The items on
these scales seem to have more face validity than the scale by Drolet,
but some factors seem more behavioral and unnecessarily specific:
pertaining to one's religiosity or biological children, whereas Lifton's
theory allows for a broader interpretation of these dimensions. The
Nature and Creative factors seem most useful and theoretically aligned
with Lifton.

Much more advanced factor analysis methods have been developed since the
1980s, but to our knowledge, these scales have not been tested with more
robust tools. The goal of the present research is to develop an
up-to-date symbolic immortality scale that more directly measures one’s
sense of symbolic immortality and which contains items more generally
applicable to broad groups of participants (e.g., regardless of a
person's religious beliefs and parental status). We have attempted to
use current best practices for scale development and analysis.

```{r libraries}
library(rio)
library(dplyr)
set.seed(8943)
library(mice)
library(psych)
library(visualizemi)
library(lavaan)

## args: dat: Dataframe of scale values, with each row being a participant and each column being a scale question
## args: rt: vector of response time/page time for each participant
## args: min: minimum value of scale
## args: max: maximum value of scale
## args: partno: vector of corresponding participant numbers
## args: clicks: click count column, outputted by qualtrics
## args: manvec: vector of manipulation check question responses
## args: mancor: correct answer of manipulation check
## args: char: number of characters on the page

SAD <- function(dat, #data frame of only scale values
               rt = NULL, #column name for page timing
               min = 1, #lower end of scale points
               max = 7, #upper end of scale points
               partno, #participant number so you can merge and identify outliers
               click = NULL, #column of click counts
               manvec = NULL, #column of manipulation check
               mancor = NULL, #answer to the manipulation check
               char = NULL){ #number of characters on the page
  
  ##make sure the data provided is a data frame
  dat = as.data.frame(dat)
  
  ####number of scale options used####
  OptUse = function(x){
    length(table(as.vector(as.matrix(unname(x)))))
  }
  numOpt = apply(dat,1,OptUse)
  numOpt = as.numeric(numOpt)
  nsim = length(numOpt)
  badScaleCheck = rep(NA, length(numOpt))
  for(i in 1:nsim){
    ##more than half of the options
    optionhalf = length(min:max)/2+1
    if(numOpt[i] >= optionhalf){
      badScaleCheck[i] = 1
    } else { badScaleCheck[i] = 0 }
  }
  badScaleCheck = as.numeric(badScaleCheck)
  
  
  ####response/page time####
  if (!is.null(rt)){
    rt = as.numeric(unlist(rt))
    
    ourchar = char
    meanchar = 987
    sdchar = 118
    upperchar = meanchar + 2*sdchar
    cutoffChar = ourchar / upperchar * 60
    badChar = rep(NA, length(rt))
    nsim = length(badChar)
    for(i in 1:nsim){
      if(!is.na(rt[i])){
        if(rt[i] < cutoffChar){
          badChar[i] = 1
        } else{
          badChar[i] = 0
        }
      } else { badChar[i] = NA}
    }
    badChar = as.numeric(badChar)
  } else { badChar = rep(NA, nrow(dat))}
  
  ####click count check####
  if(!is.null(click)){
    click = as.numeric(unlist(click))
    totalexp = apply(dat,1, function(x){sum(!is.na(x))})
    nsim = length(click)
    badClick = rep(NA, length(click))
    for(i in 1:nsim){
      if(!is.na(click[i])){
        if(click[i] >= totalexp[i]){
          badClick[i] = 0
        } else{
          badClick[i] = 1
        }
      } else{ badClick[i] = NA}
    }
    badClick = as.numeric(badClick)
  } else { badClick = rep(NA, nrow(dat))}
  
  ####manipulation check question manip####
  if(!is.null(manvec)){
    manvec = as.numeric(unlist(manvec))
    nsim = length(manvec)
    badMC = rep(NA, nrow(dat))
    for(i in 1:nsim){
      if(!is.na(manvec[i])){ 
        if(manvec[i] == mancor) { badMC[i] = 0 } else { badMC[i] = 1}} else { badMC[i] = 1}
    }
    badMC = as.numeric(badMC)
  } else { badMC = rep(NA, nrow(dat))}
  
  ####Distribution Testing####
  uniform = rep(NA,nrow(dat))
  normal = rep(NA, nrow(dat))
  dist = rep(NA, nrow(dat))
  nsim = nrow(dat)
  for(i in 1:nsim){
    temprow = as.numeric(unname(dat[i,])) 
    utable = matrix(0, nrow = 1, ncol = length(min:max))
    for(x in min:max) {
      utable[x] = length(temprow[ temprow == x])
    }
    uniformest = chisq.test(utable,
                            rescale.p = T,
                            simulate.p.value = T)
    ##test normal distribution
    ##first convert to z score
    ztest = scale(temprow)
    ##then figure out how much of the data is binned for SDs
    ztable = matrix(0, nrow = 1, ncol = 6)
    ztable[1] = length(ztest[ ztest <= -2 ])  
    ztable[2] = length(ztest[ ztest > -2 & ztest <= -1  ])
    ztable[3] = length(ztest[ ztest > -1 & ztest <= 0 ])
    ztable[4] = length(ztest[ ztest > 0 & ztest <= 1 ])
    ztable[5] = length(ztest[ ztest > 1 & ztest <= 2 ])
    ztable[6] = length(ztest[ ztest > 2 ])
    znormal = c(0.0228, 0.1359, 0.3413, 0.3413, 0.1359, 0.0228)
    normalest = chisq.test(ztable,
                           p = znormal*sum(ztable),
                           rescale.p = T,
                           simulate.p.value = T)
    ##output return chi square values
    uniform[i] = uniformest$statistic
    normal[i] = normalest$statistic
    ##if uniform < normal
    if(uniformest$statistic < normalest$statistic)
    {
      ##if more than 1 option
      if (numOpt[i]>1)
      {
        dist[i] = 0 ##uniform is better
      } else { ##handles when people only pick one thing
        dist[i] = 2
      }
    }
    ##if uniform >= normal
    if(uniformest$statistic >= normalest$statistic)
    {
      ##if more than 1 option
      if (numOpt[i]>1)
      {
        dist[i] = 1 ##normal is better
      } else { ##handles when people only pick one thing
        dist[i] = 2
      }
      
    }
    #0 means uniform fits better
    #1 means normal fits better
    #2 means only chose one scale option
  }## end of for loop
  
  ####distribution coding####
  badDist = rep(NA, nsim)
  for(i in 1:nsim){
    if(dist[i] == 0){
      badDist[i] = 1
    } 
    if (dist[i] == 1){
      badDist[i] = 0
    } 
    if (dist[i] == 2) {
      badDist[i] = 0
    }
  }
  badDist = as.numeric(badDist)
  
  ####total up####
  badDF = cbind.data.frame(badChar, badClick, badDist, badScaleCheck, badMC)
  badDF$badTotal = apply(badDF, 1, sum, na.rm = T)
  badDF$participant = partno
  
  return(badDF)
  
} #end function

percentmiss <- function(x){ sum(is.na(x))/length(x) *100 }
```

```{r load-data, eval = T}
sona <- import("../03_Data/sona_data_deidentify.csv") %>% 
  mutate(where = "sona")
sona_replaced <- import("../03_Data/sona_data_replaced.csv")
sona_page1 <- import("../03_Data/sona_page1_sad.csv")
sona_page2 <- import("../03_Data/sona_page2_sad.csv")
sona_page3 <- import("../03_Data/sona_page3_sad.csv")
sona_good <- import("../03_Data/sona_data_goodqual.csv")
sona_noout <- import("../03_Data/sona_data_noout.csv")

mturk <- import("../03_Data/mturk_data_deidentify.csv") %>% 
  mutate(where = "mturk")
mturk_replaced <- import("../03_Data/mturk_data_replaced.csv")
mturk_page1 <- import("../03_Data/mturk_page1_sad.csv")
mturk_page2 <- import("../03_Data/mturk_page2_sad.csv")
mturk_page3 <- import("../03_Data/mturk_page3_sad.csv")
mturk_good <- import("../03_Data/mturk_data_goodqual.csv")
mturk_noout <- import("../03_Data/mturk_data_noout.csv")

questions <- import("../03_Data/questions.xlsx")
```

# Method

## Participants

```{r get-age}
sona <- sona %>% 
  mutate(Q13 = gsub("[a-z]", "", Q13)) %>% 
  mutate(Q13 = ifelse(
    Q13 == 1991, 2018-1991, Q13)
  ) %>% 
  mutate(Q13 = as.numeric(Q13))

mturk <- mturk %>% 
  mutate(Q14 = as.numeric(Q14))
```

Participants were recruited from two data sources. `r nrow(sona)`
undergraduate students taking introduction to psychology were recruited
for course credit at a large midwestern university. Undergraduate
participants were *M* = `r apa_num(mean(sona$Q13, na.rm = T))` (*SD* =
`r apa_num(sd(sona$Q13, na.rm = T))`) years old at the time of the
study. `r nrow(mturk)` participants were recruited from Amazon's
Mechanical Turk, and these participants were *M* =
`r apa_num(mean(mturk$Q14, na.rm = T))` (*SD* =
`r apa_num(sd(mturk$Q14, na.rm = T))`) years old. No other demographic
information was collected in the study.

## Material and Procedure

`r nrow(questions)-3` items were developed using previous scale
development on generativity [@drolet1990], along with theoretical factor
alignment to XXCITEXX. These items aligned with areas of biological,
physical, social, spiritual, and personal immortality. The survey was
implemented in Qualtrics. First, participants entered their age in years
for basic demographic information. The immortality items were separated
into three blocks of 20-21 questions. These three blocks of questions
were randomized across participants, and the items within each block
were also randomized. In each block, one attention check item was shown
(e.g., "Answer this question as somewhat agree."). At the end of the
study, participants were directed back to either SONA for undergraduate
course credit or Mechanical Turk for monetary
compensation[\^Unfortunately, the exact amount we paid participants is
lost to time. Given our ethics application, we believe it was 1-2 USD,
and the survey was completed in *M* =
`r apa_num(mean(mturk_good[, "Duration (in seconds)"])/60)` (*SD* =
`r apa_num(sd(mturk_good[, "Duration (in seconds)"])/60)` minutes.].

## Data analysis

This study was not pre-registered. We conducted data analysis in two
phases. The data were first screened for inattentive and outlier
responses. The data were then randomly split into an exploratory and
confirmatory dataset. We developed the XX scale by using exploratory
factor analysis (EFA) with maximum likelihood estimation and an oblique
(i.e., oblimin) rotation [@preacher2003]. Analyses were performed with
the *psych* package [@revelle2020]. We used scree plots, parallel
analysis, and theory to determine the number of factors and compared
models based on fit indices to select the best model. We use the
following fit indices: Root Mean Squared Error of Approximation [RMSEA;
@steiger2016], Root Mean Squared Residual [RMSR; @jöreskog1996],
Comparative Fit Index [CFI; @bentler1990], and the Tucker-Lewis Index
[TLI; @tucker1973]. Values close to one are expected for goodness of fit
statistics [CFI, TLI; @bentler1990; @hu1999], while lower values close
to zero are considered better models for residual statistics [RMSEA,
RMSR; @steiger2016].

We then analyzed the best model from the exploratory stage using
confirmatory factor analysis (CFA) and multigroup CFA [@kline2016;
@brown2015] on the second confirmatory dataset. The models were analyzed
with *lavaan* [@rosseel2012] and *visualizemi* [@buchanan2024] *R*
packages. Multigroup CFA can be used to determine if the scale structure
can be generalized to subsamples within the data, and thus, allowing for
the same interpretation of total or subscale scores calculated from the
scale. First, we modeled the overall structure from the EFA results to
determine fit in a new sample. The RMSEA, CFI, and TLI were used to
determine model fit along with the Standardized Root Mean Squared
Residual [SRMR; @bentler1995]. The same interpretations were included
from the exploratory stage: high fit indices close to one for goodness
of fit statistics, and low fit indices close to zero for residual fit
statistics. If the overall model fit well, we then modeled the
subsamples of student populations and Mechanical Turk populations
separately. While fit is expected to potentially degrade due to smaller
sample sizes, we expect these models to converge to demonstrate that it
is appropriate to combine models for multigroup testing.

For the multigroup CFA, we used @brown2015's suggested terminology and
steps for analysis. The *configural* model includes both subsamples
combined together into one model controlling for group structure. This
model supports the "configuration" or structure of the scale is at least
similar within each group, but does not force each group to show similar
patterns of parameter estimates. Each subsequent step constrains a set
of parameters to be equal across groups to demonstrate invariance (i.e.,
equivalence). The configural model is then compared to the *metric*
model, which sets the factor loadings for each measured item to be equal
across groups. This model supports that each item is related to the
underlying latent variable the same way for each group. If this
assumption holds, the metric model is then compared to a *scalar* model,
which sets the item intercepts (i.e., item means) to be equal across
groups. Scalar model invariance suggests that each subsample shows the
same item means and factor loadings across groups. Finally, we compared
the scalar model to the *strict* model, which constrains item residuals
(i.e., errors, variance) to be the same across groups. Invariance in
model residuals suggests that the items have the same answer
range/spread across groups.

To assess if models are invariant, we used $\Delta$ CFI \<= .01
criterion to indicate that models were equivalent [@cheung2002]. If
models were not considered invariant, we investigated for the source of
the invariance by using partial invariance testing [@byrne1989;
@cheung1999]. Each parameter estimate is individually relaxed to
determine if that particular estimate leads to misfit in the model. For
example, if the model fails scalar invariance, each item mean would be
allowed to freely vary between groups (one at a time) to find the items
that show group differences. If only a few items show group differences,
then the model is considered "partially" invariant, and the impact on
total scores can be assessed. Partially invariant models indicate areas
for further investigation into group differences and ways that the final
scores of the model must be carefully interpreted. Previously, @nye2011
and @nye2019 proposed an effect size $d_{MACS}$ to help quantify the
size of the difference between groups in partially invariant models.
However, $d_{MACS}$ can generally only be used to understand the item
means and loadings together (due to the mathematical calculation),
regardless of the tyep of non-invariance. Therefore, we applied a new
technique proposed by @buchanan2024 to assess several potential effect
size components of the model: 1) overall model replication likelihood,
2) individual parameter replication likelihood, and 3) effect size for
individual parameters showing non-invariance.

We will use the $h_{nmi}$ effect size as the likelihood of replication
statistic for both the overall model and individual parameters. First,
the data is bootstrapped with replacement *n* times to create similar
samples to the original data. The grouping variable is then randomized
to create a comparison model. The proportion of times that invariance
(or non-invariance) is met is calculated for the bootstrapped and
randomized group labels. $h_{nmi}$ is Cohen's $h$ , which is the
standardized difference between two proportions for non-invariance (no
measurement invariance, but the statistic can also be calculated as
$h_{mi}$ for the difference in proportions for when the model showed
measurement invariance). We suggest $h_{nmi}$ because the interpretation
is a bit simpler: effect sizes close to zero indicate that the
bootstrapped data and randomized groups data show the same number of
non-measurement invariance findings, indicating that they are likely the
same result. The logic here is that if groups are truly equal in their
measurement on a scale, then the grouping variable should not affect the
results, thus, randomizing the grouping variable label does not affect
the results. As $h_{nmi}$ increases, we expect to find non-measurement
invariance between groups, as the bootstrapped data is consistently
different than randomized group labels. $h$ can be interpreted much like
$d$, but $d$ does not technically have upper or lower limits (i.e., they
are $\infty$), while $h$'s range is limited to $\pi$. Therefore, we will
report $h_{nmi_p}$ which is scaled to range from -1 to +1 for easier
interpretation. In addition to this effect size, we can calculate $d$
for the differences in parameters for bootstrapped versus random group
labels, thus, providing an estimate for the size of group differences on
partially invariant parameter estimates.

# Results

## Data Screening

### Undergraduate Data

Data were screened for accuracy, missing data, outliers, and multivariate parametric assumptions. 

`r nrow(sona)`
`r nrow(sona_replaced)`
`r nrow(sona_good)`
`r nrow(sona_noout)`

```{r screen-sona-accuracy-out, eval = F}
# Accuracy ----
summary(sona %>% 
          select(Q2_1:Q2_21, Q4_1:Q4_20, Q5_1:Q5_20))
apply(sona %>% 
          select(Q2_1:Q2_21, Q4_1:Q4_20, Q5_1:Q5_20), 
      2, mean, na.rm = TRUE)
apply(sona %>% 
          select(Q2_1:Q2_21, Q4_1:Q4_20, Q5_1:Q5_20), 
      2, sd, na.rm = TRUE)
# everything is within the expected range

# Missing ----

# rows
missing <- apply(sona %>% 
          select(Q2_1:Q2_21, Q4_1:Q4_20, Q5_1:Q5_20), 
          1, 
          percentmiss) 
table(missing)
replacepeople <- subset(sona, missing <= 5)
dontpeople <- subset(sona, missing > 5)

# columns
missingcol <- apply(replacepeople %>% 
                      select(Q2_1:Q2_21, Q4_1:Q4_20, Q5_1:Q5_20), 
                    2, 
                    percentmiss)
table(missingcol) # all replaceable

replacecolumn <- replacepeople %>% 
  select(Q2_1:Q2_21, Q4_1:Q4_20, Q5_1:Q5_20)
dontcolumn <- replacepeople %>% 
  select(-c(Q2_1:Q2_21, Q4_1:Q4_20, Q5_1:Q5_20))

# replacing those with <= 5% missing by multiple imputation
tempnomiss <- mice(replacecolumn)
nomiss <- complete(tempnomiss, 1)
summary(nomiss)

# putting data back together
# excluding participants with > 5% missing (7 total)
allcolumns <- cbind(dontcolumn, nomiss)
summary(allcolumns)
write.csv(allcolumns, "../03_Data/sona_data_replaced.csv", row.names = F)
```

```{r screen-sona-SAD, eval = F}
# SAD 
nomissing <- import("../03_Data/sona_data_replaced.csv")

# Block 1
page1 <- SAD(dat = nomissing %>% 
               select(Q2_1:Q2_21),
    rt = nomissing$`Q10_Page Submit`,
    min = 1, 
    max = 5, 
    partno = nomissing$ResponseId, 
    click = nomissing$`Q10_Click Count`, 
    manvec = nomissing$Q2_21, 
    mancor = 1, 
    char = 1626)
write.csv(page1, "../03_Data/sona_page1_sad.csv", row.names = F)

# Block 2
page2 <- SAD(dat = nomissing %>% 
               select(Q4_1:Q4_20), 
            rt = nomissing$`Q11_Page Submit`,
            min = 1, 
            max = 5, 
            partno = nomissing$ResponseId, 
            click = nomissing$`Q11_Click Count`, 
            manvec = nomissing$Q4_20, 
            mancor = 4, 
            char = 1491)
write.csv(page2, "../03_Data/sona_page2_sad.csv", row.names = F)

# Block 3
page3 <- SAD(dat = nomissing %>% 
               select(Q5_1:Q5_20), 
            rt = nomissing$`Q12_Page Submit`,
            min = 1, 
            max = 5, 
            partno = nomissing$ResponseId, 
            click = nomissing$`Q12_Click Count`, 
            manvec = nomissing$Q5_16, 
            mancor = 3, 
            char = 1496)
write.csv(page3, "../03_Data/sona_page3_sad.csv", row.names = F)

# Total
nomissing$totalbad <- page1$badTotal + page2$badTotal + page3$badTotal
table(nomissing$totalbad) 
sum(nomissing$totalbad < 15*.4) 
nrow(nomissing)

nomissing$totalbadUP <- 
  page1$badChar + page1$badClick + page1$badMC + 
  page2$badChar + page2$badClick + page2$badMC + 
  page3$badChar + page3$badClick + page3$badMC
table(nomissing$totalbadUP)
sum(nomissing$totalbadUP < 9*.4) 
# B&S say 2/5 which is 40%, so 40% of the 9 total is 3.6 or 4 or more

nolowqual <- subset(nomissing, totalbadUP < 4)
write.csv(nolowqual, "../03_Data/sona_data_goodqual.csv", row.names = F)
```

```{r sona-outliers, eval = F}
nolowqual <- import("../03_Data/sona_data_goodqual.csv")

mahal <- mahalanobis(nolowqual %>% select(Q2_1:Q5_20),
                     colMeans(nolowqual %>% select(Q2_1:Q5_20), na.rm = TRUE),
                     cov(nolowqual %>% select(Q2_1:Q5_20), use="pairwise.complete.obs"))

cutoff <- qchisq(1 - .001,
                 ncol(nolowqual %>% select(Q2_1:Q5_20)))
ncol(nolowqual %>% select(Q2_1:Q5_20)) #df 61
cutoff #cutoff 100.8879

summary(mahal < cutoff)

noout <- subset(nolowqual, mahal < cutoff)
write.csv(noout, "../03_Data/sona_data_noout.csv", row.names = F)
```

```{r sona-assumptions, eval = F}
random <- rchisq(nrow(sona_good), 7) 
fake <- lm(random ~ ., data = sona_good %>% 
             select(Q2_1:Q5_20)  %>% 
             select(-Q2_21, -Q4_20, -Q5_16))
standardized <- rstudent(fake)
fitvalues <- scale(fake$fitted.values)

# Linearity ----
{
  qqnorm(standardized)
  abline(0,1)
}

# Normality -----
hist(standardized, breaks=15)

# Homoscedasticity -----
{
  plot(fitvalues, standardized)
  abline(0,0) 
  abline(v = 0)
} 

random <- rchisq(nrow(sona_noout), 7) 
fake <- lm(random ~ ., data = sona_noout %>% 
             select(Q2_1:Q5_20)  %>% 
             select(-Q2_21, -Q4_20, -Q5_16))
standardized <- rstudent(fake)
fitvalues <- scale(fake$fitted.values)

# Linearity -----
{
  qqnorm(standardized)
  abline(0,1)
}

# Normality -----
hist(standardized, breaks=15)

# Homoscedasticity -----
{
  plot(fitvalues, standardized)
  abline(0,0) 
  abline(v = 0)
} 
```

### Mechanical Turk Data

`r nrow(mturk)`
`r nrow(mturk_replaced)`
`r nrow(mturk_good)`
`r nrow(mturk_noout)`

```{r screen-mturk-accuracy-out, eval = F}
# Accuracy ----
summary(mturk %>% 
          select(Q2_1:Q2_21, Q4_1:Q4_20, Q5_1:Q5_20))
apply(mturk %>% 
          select(Q2_1:Q2_21, Q4_1:Q4_20, Q5_1:Q5_20), 
      2, mean, na.rm = TRUE)
apply(mturk %>% 
          select(Q2_1:Q2_21, Q4_1:Q4_20, Q5_1:Q5_20), 
      2, sd, na.rm = TRUE)
# everything is within the expected range

# Missing ----

# rows
missing <- apply(mturk %>% 
          select(Q2_1:Q2_21, Q4_1:Q4_20, Q5_1:Q5_20), 
          1, 
          percentmiss) 
table(missing)
replacepeople <- subset(mturk, missing <= 5)
dontpeople <- subset(mturk, missing > 5)

# columns
missingcol <- apply(replacepeople %>% 
                      select(Q2_1:Q2_21, Q4_1:Q4_20, Q5_1:Q5_20), 
                    2, 
                    percentmiss)
table(missingcol) # all replaceable

replacecolumn <- replacepeople %>% 
  select(Q2_1:Q2_21, Q4_1:Q4_20, Q5_1:Q5_20)
dontcolumn <- replacepeople %>% 
  select(-c(Q2_1:Q2_21, Q4_1:Q4_20, Q5_1:Q5_20))

# replacing those with <= 5% missing by multiple imputation
tempnomiss <- mice(replacecolumn)
nomiss <- complete(tempnomiss, 1)
summary(nomiss)

# putting data back together
# excluding participants with > 5% missing (7 total)
allcolumns <- cbind(dontcolumn, nomiss)
summary(allcolumns)
write.csv(allcolumns, "../03_Data/mturk_data_replaced.csv", row.names = F)
```

```{r screen-mturk-SAD, eval = F}
# SAD 
nomissing <- import("../03_Data/mturk_data_replaced.csv")

# Block 1
page1 <- SAD(dat = nomissing %>% 
               select(Q2_1:Q2_21),
    rt = nomissing$`Q10_Page Submit`,
    min = 1, 
    max = 5, 
    partno = nomissing$ResponseId, 
    click = nomissing$`Q10_Click Count`, 
    manvec = nomissing$Q2_21, 
    mancor = 1, 
    char = 1626)
write.csv(page1, "../03_Data/mturk_page1_sad.csv", row.names = F)

# Block 2
page2 <- SAD(dat = nomissing %>% 
               select(Q4_1:Q4_20), 
            rt = nomissing$`Q11_Page Submit`,
            min = 1, 
            max = 5, 
            partno = nomissing$ResponseId, 
            click = nomissing$`Q11_Click Count`, 
            manvec = nomissing$Q4_20, 
            mancor = 4, 
            char = 1491)
write.csv(page2, "../03_Data/mturk_page2_sad.csv", row.names = F)

# Block 3
page3 <- SAD(dat = nomissing %>% 
               select(Q5_1:Q5_20), 
            rt = nomissing$`Q12_Page Submit`,
            min = 1, 
            max = 5, 
            partno = nomissing$ResponseId, 
            click = nomissing$`Q12_Click Count`, 
            manvec = nomissing$Q5_16, 
            mancor = 3, 
            char = 1496)
write.csv(page3, "../03_Data/mturk_page3_sad.csv", row.names = F)

# Total
nomissing$totalbad <- page1$badTotal + page2$badTotal + page3$badTotal
table(nomissing$totalbad) 
sum(nomissing$totalbad < 15*.4) 
nrow(nomissing)

nomissing$totalbadUP <- 
  page1$badChar + page1$badClick + page1$badMC + 
  page2$badChar + page2$badClick + page2$badMC + 
  page3$badChar + page3$badClick + page3$badMC
table(nomissing$totalbadUP)
sum(nomissing$totalbadUP < 9*.4) 
# B&S say 2/5 which is 40%, so 40% of the 9 total is 3.6 or 4 or more

nolowqual <- subset(nomissing, totalbadUP < 4)
write.csv(nolowqual, "../03_Data/mturk_data_goodqual.csv", row.names = F)
```

```{r mturk-outliers, eval = F}
nolowqual <- import("../03_Data/mturk_data_goodqual.csv")

mahal <- mahalanobis(nolowqual %>% select(Q2_1:Q5_20),
                     colMeans(nolowqual %>% select(Q2_1:Q5_20), na.rm = TRUE),
                     cov(nolowqual %>% select(Q2_1:Q5_20), use="pairwise.complete.obs"))

cutoff <- qchisq(1 - .001,
                 ncol(nolowqual %>% select(Q2_1:Q5_20)))
ncol(nolowqual %>% select(Q2_1:Q5_20)) #df 61
cutoff #cutoff 100.8879

summary(mahal < cutoff)

noout <- subset(nolowqual, mahal < cutoff)
write.csv(noout, "../03_Data/mturk_data_noout.csv", row.names = F)
```

```{r mturk-assumptions, eval = F}
random <- rchisq(nrow(mturk_good), 7) 
fake <- lm(random ~ ., data = mturk_good %>% 
             select(Q2_1:Q5_20)  %>% 
             select(-Q2_21, -Q4_20, -Q5_16))
standardized <- rstudent(fake)
fitvalues <- scale(fake$fitted.values)

# Linearity ----
{
  qqnorm(standardized)
  abline(0,1)
}

# Normality -----
hist(standardized, breaks=15)

# Homoscedasticity -----
{
  plot(fitvalues, standardized)
  abline(0,0) 
  abline(v = 0)
} 

random <- rchisq(nrow(mturk_noout), 7) 
fake <- lm(random ~ ., data = mturk_noout %>% 
             select(Q2_1:Q5_20)  %>% 
             select(-Q2_21, -Q4_20, -Q5_16))
standardized <- rstudent(fake)
fitvalues <- scale(fake$fitted.values)

# Linearity -----
{
  qqnorm(standardized)
  abline(0,1)
}

# Normality -----
hist(standardized, breaks=15)

# Homoscedasticity -----
{
  plot(fitvalues, standardized)
  abline(0,0) 
  abline(v = 0)
} 
```

```{r data-combine-split}
combo <- bind_rows(sona_good %>% 
                     mutate(where = "Undergraduate"), 
                   mturk_good %>% 
                     mutate(where = "Mechanical Turk"))
combo_noout <- bind_rows(sona_noout %>% 
                     mutate(where = "Undergraduate"), 
                     mturk_noout %>% 
                     mutate(where = "Mechanical Turk"))

random_split <- c(
  sample(sona_good$ResponseId, nrow(sona_good)/2),
  sample(mturk_good$ResponseId, round(nrow(mturk_good)/2))
)

efaDF <- combo %>% 
  filter(ResponseId %in% random_split) %>% 
  select(Q2_1:Q5_20)  %>% 
  select(-Q2_21, -Q4_20, -Q5_16)
cfaDF <- combo %>% 
  filter(!(ResponseId %in% random_split)) %>% 
  select(Q2_1:Q5_20, where)  %>% 
  select(-Q2_21, -Q4_20, -Q5_16)

nrow(efaDF)
nrow(cfaDF)

efaDF_noout <- combo_noout %>% 
  filter(ResponseId %in% random_split) %>% 
  select(Q2_1:Q5_20)  %>% 
  select(-Q2_21, -Q4_20, -Q5_16)
cfaDF_noout <- combo_noout %>% 
  filter(!(ResponseId %in% random_split)) %>% 
  select(Q2_1:Q5_20, where)  %>% 
  select(-Q2_21, -Q4_20, -Q5_16)

nrow(efaDF_noout)
nrow(cfaDF_noout)

write.csv(efaDF, "../03_Data/efaDF.csv", row.names = F)
write.csv(cfaDF, "../03_Data/cfaDF.csv", row.names = F)
```

## EFA

### Number of Factors

```{r number-factors}
#efaDF <- read.csv("../03_Data/efaDF.csv")

number_items <- fa.parallel(efaDF, #data frame
                            fm="ml", #math
                            fa="fa") #only efa
sum(number_items$fa.values > 1)
sum(number_items$fa.values > .7)
```

### Simple Structure

```{r two-factor}
item_text <- read.csv("../03_Data/item_text.csv")

EFA_fit <- fa(efaDF, #data
              nfactors = 2, #number of factors
              rotate = "oblimin", #rotation
              fm = "ml") #math

EFA_fit

# remember you gotta fix this because of the order of operations 
efa_loadings <- as.data.frame(unclass(EFA_fit$loadings)) %>% 
  mutate(keep = ifelse(
    ML1 >= .30, "yes", ifelse(
      ML2 >= .30, "yes", ifelse(
        ML1 >= .30 & ML2 >= .30, "no", "no"
      )
    )
  )) %>% 
  mutate(question = rownames(.))

EFA_fit_2 <- fa(efaDF %>% 
                  select(efa_loadings %>% 
                           filter(keep == "yes") %>% 
                           pull(question)), #data
              nfactors = 2, #number of factors
              rotate = "oblimin", #rotation
              fm = "ml") #math

EFA_fit_2

efa_loadings_2 <- as.data.frame(unclass(EFA_fit_2$loadings)) %>% 
  mutate(keep = ifelse(
    ML1 >= .30, "yes", ifelse(
      ML2 >= .30, "yes", ifelse(
        ML1 >= .30 & ML2 >= .30, "no", "no"
      )
    )
  )) %>% 
  mutate(question = rownames(.)) %>% 
  left_join(questions,
            by = c("question" = "Question_Number")) %>% 
  arrange(desc(ML1)) %>% 
  export(., "results/2_factor.xlsx", row.names = F)
```

```{r three-factor}

EFA_fit <- fa(efaDF, #data
              nfactors = 3, #number of factors
              rotate = "oblimin", #rotation
              fm = "ml") #math

EFA_fit

efa_loadings <- as.data.frame(unclass(EFA_fit$loadings)) %>% 
  mutate(keep = ifelse(
    ML1 >= .30, "yes", ifelse(
      ML2 >= .30, "yes", ifelse(
        ML3 >= .30, "yes", "no"
        )
      )
    )
  ) %>% 
  mutate(question = rownames(.)) %>% 
  mutate(
    keep = ifelse(
          (ML1 >= .30 & ML2 >= .30) | 
            (ML1 >= .30 & ML3 >= .30) |
            (ML3 >= .30 & ML2 >= .30), "no", keep
          )
  )

EFA_fit_2 <- fa(efaDF %>% 
                  select(efa_loadings %>% 
                           filter(keep == "yes") %>% 
                           pull(question)), #data
              nfactors = 3, #number of factors
              rotate = "oblimin", #rotation
              fm = "ml") #math

EFA_fit_2

efa_loadings_2 <- as.data.frame(unclass(EFA_fit_2$loadings)) %>% 
  mutate(keep = ifelse(
    ML1 >= .30, "yes", ifelse(
      ML2 >= .30, "yes", ifelse(
        ML3 >= .30, "yes", "no"
        )
      )
    )
  ) %>% 
  mutate(question = rownames(.)) %>% 
  mutate(
    keep = ifelse(
          (ML1 >= .30 & ML2 >= .30) | 
            (ML1 >= .30 & ML3 >= .30) |
            (ML3 >= .30 & ML2 >= .30), "no", keep
          )
  ) %>% 
  left_join(questions,
            by = c("question" = "Question_Number")) %>% 
  arrange(desc(ML1)) %>% 
  export(., "results/3_factor.xlsx", row.names = F)
```

```{r five-factor}
EFA_fit <- fa(efaDF, #data
              nfactors = 5, #number of factors
              rotate = "oblimin", #rotation
              fm = "ml") #math

EFA_fit

efa_loadings <- as.data.frame(unclass(EFA_fit$loadings)) %>% 
  mutate(question = rownames(.)) %>% 
  rowwise() %>%
  mutate(keep = sum(c_across(-question) >= .30)) %>% 
  ungroup() 

EFA_fit_2 <- fa(efaDF %>% 
                  select(efa_loadings %>% 
                           filter(keep == 1) %>% 
                           pull(question)), #data
              nfactors = 5, #number of factors
              rotate = "oblimin", #rotation
              fm = "ml") #math

EFA_fit_2

efa_loadings_2 <- as.data.frame(unclass(EFA_fit_2$loadings)) %>% 
  mutate(question = rownames(.)) %>% 
  rowwise() %>%
  mutate(keep = sum(c_across(-question) >= .30)) %>% 
  ungroup() 

EFA_fit_3 <- fa(efaDF %>% 
                  select(efa_loadings_2 %>% 
                           filter(keep == 1) %>% 
                           pull(question)), #data
              nfactors = 5, #number of factors
              rotate = "oblimin", #rotation
              fm = "ml") #math

EFA_fit_3

efa_loadings_3 <- as.data.frame(unclass(EFA_fit_3$loadings)) %>% 
  mutate(question = rownames(.)) %>% 
  rowwise() %>%
  mutate(keep = sum(c_across(-question) >= .30)) %>% 
  ungroup() %>% 
  left_join(questions,
            by = c("question" = "Question_Number")) %>% 
  arrange(desc(ML1)) %>% 
  export(., "results/5_factor.xlsx", row.names = F)
```

```{r comparison-models}
efa_results <- import("results/final_questions.xlsx")
EFA_2factor <- fa(efaDF %>% 
                    select(efa_results %>% 
                           filter(!is.na(factor2)) %>% 
                           pull(question)),
                  nfactors = 2,
                  rotate = "oblimin", #rotation
                  fm = "ml") #math

EFA_3factor <- fa(efaDF %>% 
                    select(efa_results %>% 
                           filter(!is.na(factor3)) %>% 
                           pull(question)),
                  nfactors = 3,
                  rotate = "oblimin", #rotation
                  fm = "ml") #math

EFA_5factor <- fa(efaDF %>% 
                    select(efa_results %>% 
                           filter(!is.na(factor5)) %>% 
                           pull(question)),
                  nfactors = 5,
                  rotate = "oblimin", #rotation
                  fm = "ml") #math

EFA_2factor
EFA_3factor
EFA_5factor

new_5_columns <- as.data.frame(unclass(EFA_5factor$loadings)) %>% 
  mutate(question = rownames(.)) %>% 
  rowwise() %>%
  mutate(keep = sum(c_across(-question) >= .30)) %>% 
  ungroup() %>% 
  arrange(desc(keep)) %>% 
  filter(keep == 1) %>% 
  pull(question)

EFA_5factor <- fa(efaDF %>% 
                    select(all_of(new_5_columns)),
                  nfactors = 5,
                  rotate = "oblimin", #rotation
                  fm = "ml") #math

EFA_5factor

new_5_columns <- as.data.frame(unclass(EFA_5factor$loadings)) %>% 
  mutate(question = rownames(.)) %>% 
  rowwise() %>%
  mutate(keep = sum(c_across(-question) >= .30)) %>% 
  ungroup() %>% 
  arrange(desc(keep)) %>% 
  filter(keep == 1) %>% 
  pull(question)

EFA_5factor <- fa(efaDF %>% 
                    select(all_of(new_5_columns)),
                  nfactors = 5,
                  rotate = "oblimin", #rotation
                  fm = "ml") #math

EFA_5factor

# as.data.frame(unclass(EFA_5factor$loadings)) %>% 
#   mutate(question = rownames(.)) %>% 
#   rowwise() %>%
#   mutate(keep = sum(c_across(-question) >= .30)) %>% 
#   ungroup() %>% 
#   left_join(questions,
#             by = c("question" = "Question_Number")) %>% 
#   arrange(desc(ML1)) %>% 
#   export(., "results/5_factor_reduced.xlsx", row.names = F)
```

```{r}
# test if q2_1 changing to a different place changes results 
five_reduced <- import('results/5_factor_reduced.xlsx')
five.model <- ""

for (factor in unique(five_reduced$Factor)){
  questions <- five_reduced %>% 
    filter(Factor == factor) %>% 
    pull(question)
  
  five.model <- 
    paste0(five.model, "\n",
        factor, " =~ ", 
        paste(questions, collapse = " + "))
}

#cat(five.model)

model.results <- mgcfa(
  model = five.model,
  data = cfaDF,
  group = "where", 
  group.equal = c("loadings", "intercepts", "residuals"),
  meanstructure = TRUE
)

summary(model.results$model_overall, 
        standardized = T,
        fit.measures = T, 
        rsquare = T)

summary(model.results$group_models$model.Undergraduate, 
        standardized = T,
        fit.measures = T, 
        rsquare = T)

modificationindices(model.results$group_models$model.Undergraduate,
                    sort. = T)

summary(model.results$group_models$`model.Mechanical Turk`, 
        standardized = T,
        fit.measures = T, 
        rsquare = T)

modificationindices(model.results$group_models$`model.Mechanical Turk`, 
                    sort. = T)

summary(model.results$model_configural, 
        standardized = T,
        fit.measures = T, 
        rsquare = T)

model.results$model_fit

model.partial <- partial_mi(
  saved_model = model.results$invariance_models$model.intercepts,
  data = cfaDF,
  model = five.model,
  group = "where",
  group.equal = c("loadings", "intercepts"),
  partial_step = "intercepts")

model.partial$fit_table
```

```{r}
# distribution of each latent variable as a plot 

saved_mi_plots <- plot_mi(data_coef = model.results.partial$model_coef,
                          # which model do you want to plot from model column
                          model_step = "intercepts", 
                          # name of observed item
                          item_name = "Q4_8", 
                          # LV limits to graph
                          x_limits = c(-2,2), 
                          # Y min and max in data
                          y_limits = c(min(cfaDF$Q4_8),
                                       max(cfaDF$Q4_8)), 
                          # what ci do you want
                          conf.level = .95, 
                          # what model results do you want
                          model_results = model.results.partial$invariance_models$model.intercepts, 
                          # which latent is the observed variable on
                          # important for cross-loaded variables
                          lv_name = "biological", 
                          # if you have more than two groups, which two do you want
                          plot_groups = NULL)

saved_mi_plots$complete

saved_mi_plots <- plot_mi(data_coef = model.results.partial$model_coef,
                          # which model do you want to plot from model column
                          model_step = "intercepts", 
                          # name of observed item
                          item_name = "Q2_4", 
                          # LV limits to graph
                          x_limits = c(-1,1), 
                          # Y min and max in data
                          y_limits = c(min(cfaDF$Q2_4),
                                       max(cfaDF$Q2_4)), 
                          # what ci do you want
                          conf.level = .95, 
                          # what model results do you want
                          model_results = model.results.partial$invariance_models$model.intercepts, 
                          # which latent is the observed variable on
                          # important for cross-loaded variables
                          lv_name = "social", 
                          # if you have more than two groups, which two do you want
                          plot_groups = NULL)

model.results.partial <- mgcfa(
  model = five.model,
  data = cfaDF,
  group = "where", 
  group.equal = c("loadings", "intercepts", "residuals"),
  meanstructure = TRUE,
  group.partial = "Q4_8 ~1"
)

# figure out how to update this 
# so that it will take the group.partial correctly 
model.partial.partial <- partial_mi(
  saved_model = model.results.partial$invariance_models$model.residuals,
  data = cfaDF,
  model = five.model,
  group = "where",
  group.equal = c("loadings", "intercepts", "residuals"),
  partial_step = "residuals")

model.partial.partial$fit_table
```

```{r boot-model, eval = F}
saved_boot_model <- bootstrap_model(
  # saved configural model to start at
  saved_configural = model.results$model_configural,
  # dataset for the analysis
  data = cfaDF,
  # model lavaan syntax
  model = five.model,
  # group variable in the dataset
  group = "where", 
  # number of bootstraps
  # this is set to a low number to compile quickly for cran
  nboot = 1000,
  # name of the fit measure you want to use, make sure it's lavaan
  invariance_index = "cfi",
  # rule for the difference in fit indices
  invariance_rule = .01,
  # what order of steps do you want to test? 
  group.equal = c("loadings", "intercepts", "residuals")
)

saveRDS(saved_boot_model, "../03_Data/saved_boot_model.rds")
```

```{r boot-partial, eval = F}
saved_boot_partial <- bootstrap_partial(
  # the model you want to test 
  # use the model before your invariant one 
  # similar set up to partial_mi
  saved_model = model.results$invariance_models$model.loadings,
  # the dataframe
  data = cfaDF,
  # the original model syntax
  model = five.model,
  # the grouping variable column
  group = "where",
  # run more, but this package vignette needs to knit fast
  nboot = 1000,
  # what index are you using for invariance?
  # match this to lavaan's name under fitmeasures()
  invariance_index = "cfi",
  # what rule are you using?
  invariance_rule = .01,
  # what are we comparing against? 
  invariance_compare = unname(fitmeasures(model.results$invariance_models$model.loadings, "cfi")),
  # which step you want to estimate effect size for
  partial_step = c("intercepts"), 
  # which parameters you want to hold constrained
  group.equal = c("loadings", "intercepts")
  )
saveRDS(saved_boot_partial, "../03_Data/saved_boot_partial.rds")
```

# Discussion

\newpage

# References

::: {#refs custom-style="Bibliography"}
:::
