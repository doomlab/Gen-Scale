---
title             : "The title"
shorttitle        : "Title"
author: 
  - name          : "First Author"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Postal address"
    email         : "my@email.com"
    role: # Contributorship roles (e.g., CRediT, https://credit.niso.org/)
      - "Conceptualization"
      - "Writing - Original Draft Preparation"
      - "Writing - Review & Editing"
  - name          : "Ernst-August Doelle"
    affiliation   : "1,2"
    role:
      - "Writing - Review & Editing"
      - "Supervision"
affiliation:
  - id            : "1"
    institution   : "Wilhelm-Wundt-University"
  - id            : "2"
    institution   : "Konstanz Business School"
authornote: |
  Add complete departmental affiliations for each author here. Each new line herein must be indented, like this line.
  Enter author note here.
abstract: |
  One or two sentences providing a **basic introduction** to the field,  comprehensible to a scientist in any discipline.
  Two to three sentences of **more detailed background**, comprehensible  to scientists in related disciplines.
  One sentence clearly stating the **general problem** being addressed by  this particular study.
  One sentence summarizing the main result (with the words "**here we show**" or their equivalent).
  Two or three sentences explaining what the **main result** reveals in direct comparison to what was thought to be the case previously, or how the  main result adds to previous knowledge.
  One or two sentences to put the results into a more **general context**.
  Two or three sentences to provide a **broader perspective**, readily comprehensible to a scientist in any discipline.
  
  
  <!-- https://tinyurl.com/ybremelq -->
keywords          : "keywords"
wordcount         : "X"
floatsintext      : no
linenumbers       : yes
draft             : no
mask              : no
figurelist        : no
tablelist         : no
footnotelist      : no
classoption       : "man"
output            : papaja::apa6_pdf
bibliography: references.bib
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include = FALSE}
library("papaja")
r_refs("r-references.bib")
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

```{r trackdown, eval = F}
library(trackdown)
# upload_file(
#   file = "manuscript.Rmd",
#   gfile = "gen_scale_manuscript_trackdown",
#   gpath = "gen_scale/",
#   hide_code = TRUE
# )

# pull to me 
download_file(
  file = "manuscript.Rmd",
  gfile = "gen_scale_manuscript_trackdown",
  gpath = "gen_scale/",
  hide_code = TRUE
)

# push to google
update_file(
  file = "manuscript.Rmd",
  gfile = "gen_scale_manuscript_trackdown",
  gpath = "gen_scale/",
  hide_code = TRUE
)
```

Death is something each of us must learn to cope with, whether in
healthy ways or less so. These issues may be at front of mind for many
in light of the COVID-19 pandemic. Various existential philosophers and
psychologists have proposed ways in which we deal with the awareness of
death and the anxiety this awareness often causes. Psychoanalyst Erik
Erikson [-@erikson1950] proposed that during mid-life one becomes
acutely aware of their oncoming death and is motivated to care for
things which will outlast themselves. He called this act of caring
generativity. In *The Denial of Death*, philosopher Ernest Becker
[-@becker1973] posits that humans undertake immortality projects to curb
their sense of vulnerability to death. Similarly, psychiatrist Robert
Jay Lifton [-@lifton1979], a mentee of Erikson, described the awareness
of death as being ever present and motivating us to create symbols,
thereby allowing us to imagine ourselves as symbolically immortalized.
Existential psychiatrist Irvin Yalom [-@yalom2008] notes that many of
his clients experiencing anxiety about their death take comfort in
“rippling,” the idea that one’s lasting effects on the world will ripple
out and influence the world after they have died.

Although these thinkers use different terminology, there are several
common themes among their ideas. (1) Our physical death is an
inevitability, and we often find our awareness of its inevitability to
be aversive. This aversion may be referred to variously as angst,
death-anxiety, despair, being-towards-death, terror, and so on. However,
(2) we take comfort in the idea that other, non-physical parts of us
continue to exist indefinitely after our biological death, through
mechanisms such as the heroic archetype and symbolic self. (3) Finally,
we can take action to promote these non-physical aspects of the self,
such as through search for meaning, sense of immortality, care,
generativity, and rippling.

One of these bodies of thought, called symbolic immortality, was
originally theorized by Lifton [-@lifton1979], who thought that
awareness of death drives a fundamental human desire for a sense of
continuity lasting beyond the lifespan. Essentially, humans are
meaning-seeking creatures, and throughout our lives, this search for
meaning involves an evolving psychological imagery of life and death.
Death, or the transient nature of life, threatens our search for
meaning. Lifton thought that if we could achieve what we believe to be
some form of immortality, we could overcome this loss of meaning, and
the awareness of death could instead drive an inner vitality (imagery
associated with connection, integrity, and movement). If this drive
toward vitality is lost, we are vulnerable to a psychic numbness or
death-in-life (imagery associated with separation, disintegration, and
stasis). In Lifton's own words, "Death does indeed bring about
biological and psychic annihilation. But life includes symbolic
perceptions of connections that precede and outlast that annihilation"
[-@lifton1979, p. 18].

Lifton [-@lifton1979] proposed five modes of experience or ways of
achieving symbolic immortality: The biological (or biosocial) mode in
which one lives on through their genetic and sociocultural progeny, the
creative mode in which one’s accomplishments and contribution outlast
oneself, the natural mode in which one feels they are a part of the
broader universe, the spiritual mode in which one seeks to transcend the
physical realm to a higher spiritual realm beyond death, and the mode of
experiential transcendence in which one experiences a phenomenological
state of flow. The experiential mode must occur in the context of at
least one of the other four to really be considered transcendent, but it
is thought to have a great capacity to bring about personal change.

Claims of how we suppress death-anxiety have been investigated
experimentally, primarily through the paradigm of Terror Management
Theory (TMT). Based on the theories of Ernest Becker, TMT posits that
human awareness of death is always present to some degree. This
awareness of our inevitable death, coupled with a strong aversion to
thoughts of death, causes terror and is pushed out of our consciousness
by our creation of meaning systems [@greenberg1986]. TMT proposes that
self-esteem, interpersonal relationships, and cultural worldview work
together to buffer against our anxiety about death. It is assumed that
these buffers suppress thoughts of death by providing a sense of
symbolic immortality, though little systematic research has been
conducted on this construct. The results of this buffering process are
not always positive. For example, experimentally priming mortality
salience can lead to more positive attitudes toward in-group members but
harsher negative attitudes toward out-group members [@greenberg1990].

TMT refers to a person's awareness of death as mortality salience (MS).
The MS hypothesis of TMT posits than an increase in one's awareness of
death causes an increase in compensatory behaviors to lower their death
anxiety, either by distracting from the awareness of death or by the
promotion of meaningful cultural worldviews. In the MS paradigm,
experimentally priming a participant’s awareness of death (for example,
by having participants write about death and then complete a distraction
task) is thought to cause an increase in compensatory buffers. A
meta-analysis of 277 experiments found mortality salience to have a
robust, moderate overall effect size: *r*(276) = 0.35, *p* = .00
[@burke2010]. Altogether, these experiments provide convincing evidence
for TMT and the MS hypothesis in particular.

Though some avoidance of (or buffering against) death anxiety is thought
to be universal and has the potential to increase interpersonal
conflict, awareness of death through symbolic immortality may also have
potential as a positive force. In particular, it is thought to be an
underlying motive for what Erikson referred to as generativity.
Generativity is the seventh of eight proposed stages in Erikson's
[-@erikson1950] theory of psychosocial development, which he associated
with midlife and described as "the concern in establishing and guiding
the next generation" [@erikson1963, p. 267]. Little systematic research
was conducted on this subject until the 1980's. Kotre [-@kotre1984]
expanded on the theory and proposed that the drive for generativity was
related to a motive to expand the sense of self beyond the lifetime,
especially in light of the fear of death.

McAdams and de St. Aubin [-@mcadams1992] sought to formalize the study
of generativity as a multidimensional construct. Their seven components
of generativity include cultural demand, inner desire (for symbolic
immortality and community), concern (for the next generation), belief
(in the human species), commitment, action, and narration (of
generativity within one's life story). In addition to a quantitative
measure of generative concern (the Loyola Generativity Scale), they
developed a system for content analysis of autobiographical episodes
pertaining to generativity, and symbolic immortality is one of the five
themes they found. Here they define symbolic immortality as "any
reference to leaving a legacy, having an enduring influence, or leaving
behind products that will outlive one's physical existence," a theme
clearly related to both Lifton's and Erikson's theories [@mcadams1992,
p. 1011].

These research areas depend on the construct of symbolic immortality for
their theoretical frameworks, but few researchers have attempted to
systematically and quantitatively assess this construct. Two attempts
have been made to develop such a measurement: Drolet's [-@drolet1990]
Sense of Symbolic Immortality Scale and Mathews and Kling's
[-@mathews1988] measure of symbolic immortality, based on an original
questionnaire by Mathews and Mister [-@mathews1987].

Drolet [-@drolet1990] developed the Sense of Symbolic Immortality Scale
based on Robert J. Lifton’s theory of symbolic immortality and its five
modes of experience. Drolet studied 136 adults, ages 18-30 and 30-40,
and hypothesized that those in their 30's (established adults) would
have a greater sense of symbolic immortality than the young adults
(18-30). The measure is inherently subjective, not only by the nature of
self-report, but in that the scale seeks to measure what a person
*believes* and how they *feel* about these subjects. The scale as a
whole had a high internal consistency ($\alpha$ = .91) and test-retest
reliability was *r* = .97. Internal consistency of subscales for the
five theoretical modes of immortality was mixed. Of the five, spiritual
immortality was the most distinct from the scale as a whole and the
other subscales. Factor analysis showed three factors, mapping onto
biosocial, creative, and spiritual. The transcendent and natural items
may be closely related to biosocial.

Moving beyond the scale development itself, SSI correlated negatively
with Templer's Death Anxiety Scale and had a strong (*r* = .84) positive
relationship with Maholick's Purpose in Life Test [@drolet1990]. In
interpreting the very strong correlation, the author suggests that SSI
is a broader construct than Purpose in Life and the scale itself may be
less prone to social desirability effects than the PIL, although this
had not been directly tested. Age group was also related, with
established adults having a higher SSI, particularly in the biosocial
and creative domains.

We see multiple issues with using the Symbolic Immortality Scale. First,
the study was underpowered, conducting exploratory factor analysis of 67
items using a sample of 136. Second, the scale was developed in French,
and we do not take for granted the psychometric properties of a
translated version. Third and most fundamentally, the scale has poor
face validity and appears to measure the constructs theorized to
symbolically immortalize rather than a sense of symbolic immortality
directly. For example, the scale includes items such as “My sex life
contributes greatly to my well-being”, “Intimate relationships scare
me”, and “I am sure of who I am." Although related to the constructs
(such as interpersonal relationships and self-esteem) which
theoretically help cope with death, it is unclear how these items
represent the construct of symbolic immortality itself.

Mathews and Mister [-@mathews1987] also developed a scale pertaining to
symbolic immortality, sensation seeking, and psychic numbness in a study
including 400 adults. Experiential transcendence was operationalized as
similar to Zuckerman's [-@zuckerman1979] sensation seeking, which may
not fully capture the original intent (the experience of losing
oneself). Items were mapped onto five factors, and the five factors
largely aligned with Lifton's constructs. Although internal consistency
was at least acceptable for each factor, goodness of fit statistics are
not reported. Some studies have used a revised version of the scale by
Mathews and Kling [-@mathews1988], who adapted it for a study on
prosocial behavior in the context of nonprofit volunteer motivation.
They reported similar results for their revised scale. The items on
these scales seem to have more face validity than the scale by Drolet,
but some factors seem more behavioral and unnecessarily specific:
pertaining to one's religiosity or biological children, whereas Lifton's
theory allows for a broader interpretation of these dimensions. The
Nature and Creative factors seem most useful and theoretically aligned
with Lifton.

Much more advanced factor analysis methods have been developed since the
1980s, but to our knowledge, these scales have not been tested with more
robust tools. The goal of the present research is to develop an
up-to-date symbolic immortality scale that more directly measures one’s
sense of symbolic immortality and which contains items more generally
applicable to broad groups of participants (e.g., regardless of a
person's religious beliefs and parental status). We have attempted to
use current best practices for scale development and analysis.

# Method

## Participants

## Material

## Procedure

## Data analysis

# Results

```{r libraries}
library(rio)
library(dplyr)
set.seed(8943)
library(mice)
library(psych)

## args: dat: Dataframe of scale values, with each row being a participant and each column being a scale question
## args: rt: vector of response time/page time for each participant
## args: min: minimum value of scale
## args: max: maximum value of scale
## args: partno: vector of corresponding participant numbers
## args: clicks: click count column, outputted by qualtrics
## args: manvec: vector of manipulation check question responses
## args: mancor: correct answer of manipulation check
## args: char: number of characters on the page

SAD <- function(dat, #data frame of only scale values
               rt = NULL, #column name for page timing
               min = 1, #lower end of scale points
               max = 7, #upper end of scale points
               partno, #participant number so you can merge and identify outliers
               click = NULL, #column of click counts
               manvec = NULL, #column of manipulation check
               mancor = NULL, #answer to the manipulation check
               char = NULL){ #number of characters on the page
  
  ##make sure the data provided is a data frame
  dat = as.data.frame(dat)
  
  ####number of scale options used####
  OptUse = function(x){
    length(table(as.vector(as.matrix(unname(x)))))
  }
  numOpt = apply(dat,1,OptUse)
  numOpt = as.numeric(numOpt)
  nsim = length(numOpt)
  badScaleCheck = rep(NA, length(numOpt))
  for(i in 1:nsim){
    ##more than half of the options
    optionhalf = length(min:max)/2+1
    if(numOpt[i] >= optionhalf){
      badScaleCheck[i] = 1
    } else { badScaleCheck[i] = 0 }
  }
  badScaleCheck = as.numeric(badScaleCheck)
  
  
  ####response/page time####
  if (!is.null(rt)){
    rt = as.numeric(unlist(rt))
    
    ourchar = char
    meanchar = 987
    sdchar = 118
    upperchar = meanchar + 2*sdchar
    cutoffChar = ourchar / upperchar * 60
    badChar = rep(NA, length(rt))
    nsim = length(badChar)
    for(i in 1:nsim){
      if(!is.na(rt[i])){
        if(rt[i] < cutoffChar){
          badChar[i] = 1
        } else{
          badChar[i] = 0
        }
      } else { badChar[i] = NA}
    }
    badChar = as.numeric(badChar)
  } else { badChar = rep(NA, nrow(dat))}
  
  ####click count check####
  if(!is.null(click)){
    click = as.numeric(unlist(click))
    totalexp = apply(dat,1, function(x){sum(!is.na(x))})
    nsim = length(click)
    badClick = rep(NA, length(click))
    for(i in 1:nsim){
      if(!is.na(click[i])){
        if(click[i] >= totalexp[i]){
          badClick[i] = 0
        } else{
          badClick[i] = 1
        }
      } else{ badClick[i] = NA}
    }
    badClick = as.numeric(badClick)
  } else { badClick = rep(NA, nrow(dat))}
  
  ####manipulation check question manip####
  if(!is.null(manvec)){
    manvec = as.numeric(unlist(manvec))
    nsim = length(manvec)
    badMC = rep(NA, nrow(dat))
    for(i in 1:nsim){
      if(!is.na(manvec[i])){ 
        if(manvec[i] == mancor) { badMC[i] = 0 } else { badMC[i] = 1}} else { badMC[i] = 1}
    }
    badMC = as.numeric(badMC)
  } else { badMC = rep(NA, nrow(dat))}
  
  ####Distribution Testing####
  uniform = rep(NA,nrow(dat))
  normal = rep(NA, nrow(dat))
  dist = rep(NA, nrow(dat))
  nsim = nrow(dat)
  for(i in 1:nsim){
    temprow = as.numeric(unname(dat[i,])) 
    utable = matrix(0, nrow = 1, ncol = length(min:max))
    for(x in min:max) {
      utable[x] = length(temprow[ temprow == x])
    }
    uniformest = chisq.test(utable,
                            rescale.p = T,
                            simulate.p.value = T)
    ##test normal distribution
    ##first convert to z score
    ztest = scale(temprow)
    ##then figure out how much of the data is binned for SDs
    ztable = matrix(0, nrow = 1, ncol = 6)
    ztable[1] = length(ztest[ ztest <= -2 ])  
    ztable[2] = length(ztest[ ztest > -2 & ztest <= -1  ])
    ztable[3] = length(ztest[ ztest > -1 & ztest <= 0 ])
    ztable[4] = length(ztest[ ztest > 0 & ztest <= 1 ])
    ztable[5] = length(ztest[ ztest > 1 & ztest <= 2 ])
    ztable[6] = length(ztest[ ztest > 2 ])
    znormal = c(0.0228, 0.1359, 0.3413, 0.3413, 0.1359, 0.0228)
    normalest = chisq.test(ztable,
                           p = znormal*sum(ztable),
                           rescale.p = T,
                           simulate.p.value = T)
    ##output return chi square values
    uniform[i] = uniformest$statistic
    normal[i] = normalest$statistic
    ##if uniform < normal
    if(uniformest$statistic < normalest$statistic)
    {
      ##if more than 1 option
      if (numOpt[i]>1)
      {
        dist[i] = 0 ##uniform is better
      } else { ##handles when people only pick one thing
        dist[i] = 2
      }
    }
    ##if uniform >= normal
    if(uniformest$statistic >= normalest$statistic)
    {
      ##if more than 1 option
      if (numOpt[i]>1)
      {
        dist[i] = 1 ##normal is better
      } else { ##handles when people only pick one thing
        dist[i] = 2
      }
      
    }
    #0 means uniform fits better
    #1 means normal fits better
    #2 means only chose one scale option
  }## end of for loop
  
  ####distribution coding####
  badDist = rep(NA, nsim)
  for(i in 1:nsim){
    if(dist[i] == 0){
      badDist[i] = 1
    } 
    if (dist[i] == 1){
      badDist[i] = 0
    } 
    if (dist[i] == 2) {
      badDist[i] = 0
    }
  }
  badDist = as.numeric(badDist)
  
  ####total up####
  badDF = cbind.data.frame(badChar, badClick, badDist, badScaleCheck, badMC)
  badDF$badTotal = apply(badDF, 1, sum, na.rm = T)
  badDF$participant = partno
  
  return(badDF)
  
} #end function

percentmiss <- function(x){ sum(is.na(x))/length(x) *100 }
```

```{r load-data, eval = T}
sona <- import("../03_Data/sona_data_deidentify.csv") %>% 
  mutate(where = "sona")
sona_replaced <- import("../03_Data/sona_data_replaced.csv")
sona_page1 <- import("../03_Data/sona_page1_sad.csv")
sona_page2 <- import("../03_Data/sona_page2_sad.csv")
sona_page3 <- import("../03_Data/sona_page3_sad.csv")
sona_good <- import("../03_Data/sona_data_goodqual.csv")
sona_noout <- import("../03_Data/sona_data_noout.csv")

mturk <- import("../03_Data/mturk_data_deidentify.csv") %>% 
  mutate(where = "mturk")
mturk_replaced <- import("../03_Data/mturk_data_replaced.csv")
mturk_page1 <- import("../03_Data/mturk_page1_sad.csv")
mturk_page2 <- import("../03_Data/mturk_page2_sad.csv")
mturk_page3 <- import("../03_Data/mturk_page3_sad.csv")
mturk_good <- import("../03_Data/mturk_data_goodqual.csv")
mturk_noout <- import("../03_Data/mturk_data_noout.csv")

questions <- import("../03_Data/questions.xlsx")
```

## Data Screening

```{r screen-sona-accuracy-out, eval = F}
# Accuracy ----
summary(sona %>% 
          select(Q2_1:Q2_21, Q4_1:Q4_20, Q5_1:Q5_20))
apply(sona %>% 
          select(Q2_1:Q2_21, Q4_1:Q4_20, Q5_1:Q5_20), 
      2, mean, na.rm = TRUE)
apply(sona %>% 
          select(Q2_1:Q2_21, Q4_1:Q4_20, Q5_1:Q5_20), 
      2, sd, na.rm = TRUE)
# everything is within the expected range

# Missing ----

# rows
missing <- apply(sona %>% 
          select(Q2_1:Q2_21, Q4_1:Q4_20, Q5_1:Q5_20), 
          1, 
          percentmiss) 
table(missing)
replacepeople <- subset(sona, missing <= 5)
dontpeople <- subset(sona, missing > 5)

# columns
missingcol <- apply(replacepeople %>% 
                      select(Q2_1:Q2_21, Q4_1:Q4_20, Q5_1:Q5_20), 
                    2, 
                    percentmiss)
table(missingcol) # all replaceable

replacecolumn <- replacepeople %>% 
  select(Q2_1:Q2_21, Q4_1:Q4_20, Q5_1:Q5_20)
dontcolumn <- replacepeople %>% 
  select(-c(Q2_1:Q2_21, Q4_1:Q4_20, Q5_1:Q5_20))

# replacing those with <= 5% missing by multiple imputation
tempnomiss <- mice(replacecolumn)
nomiss <- complete(tempnomiss, 1)
summary(nomiss)

# putting data back together
# excluding participants with > 5% missing (7 total)
allcolumns <- cbind(dontcolumn, nomiss)
summary(allcolumns)
write.csv(allcolumns, "../03_Data/sona_data_replaced.csv", row.names = F)
```

```{r screen-sona-SAD, eval = F}
# SAD 
nomissing <- import("../03_Data/sona_data_replaced.csv")

# Block 1
page1 <- SAD(dat = nomissing %>% 
               select(Q2_1:Q2_21),
    rt = nomissing$`Q10_Page Submit`,
    min = 1, 
    max = 5, 
    partno = nomissing$ResponseId, 
    click = nomissing$`Q10_Click Count`, 
    manvec = nomissing$Q2_21, 
    mancor = 1, 
    char = 1626)
write.csv(page1, "../03_Data/sona_page1_sad.csv", row.names = F)

# Block 2
page2 <- SAD(dat = nomissing %>% 
               select(Q4_1:Q4_20), 
            rt = nomissing$`Q11_Page Submit`,
            min = 1, 
            max = 5, 
            partno = nomissing$ResponseId, 
            click = nomissing$`Q11_Click Count`, 
            manvec = nomissing$Q4_20, 
            mancor = 4, 
            char = 1491)
write.csv(page2, "../03_Data/sona_page2_sad.csv", row.names = F)

# Block 3
page3 <- SAD(dat = nomissing %>% 
               select(Q5_1:Q5_20), 
            rt = nomissing$`Q12_Page Submit`,
            min = 1, 
            max = 5, 
            partno = nomissing$ResponseId, 
            click = nomissing$`Q12_Click Count`, 
            manvec = nomissing$Q5_16, 
            mancor = 3, 
            char = 1496)
write.csv(page3, "../03_Data/sona_page3_sad.csv", row.names = F)

# Total
nomissing$totalbad <- page1$badTotal + page2$badTotal + page3$badTotal
table(nomissing$totalbad) 
sum(nomissing$totalbad < 15*.4) 
nrow(nomissing)

nomissing$totalbadUP <- 
  page1$badChar + page1$badClick + page1$badMC + 
  page2$badChar + page2$badClick + page2$badMC + 
  page3$badChar + page3$badClick + page3$badMC
table(nomissing$totalbadUP)
sum(nomissing$totalbadUP < 9*.4) 
# B&S say 2/5 which is 40%, so 40% of the 9 total is 3.6 or 4 or more

nolowqual <- subset(nomissing, totalbadUP < 4)
write.csv(nolowqual, "../03_Data/sona_data_goodqual.csv", row.names = F)
```

```{r sona-outliers, eval = F}
nolowqual <- import("../03_Data/sona_data_goodqual.csv")

mahal <- mahalanobis(nolowqual %>% select(Q2_1:Q5_20),
                     colMeans(nolowqual %>% select(Q2_1:Q5_20), na.rm = TRUE),
                     cov(nolowqual %>% select(Q2_1:Q5_20), use="pairwise.complete.obs"))

cutoff <- qchisq(1 - .001,
                 ncol(nolowqual %>% select(Q2_1:Q5_20)))
ncol(nolowqual %>% select(Q2_1:Q5_20)) #df 61
cutoff #cutoff 100.8879

summary(mahal < cutoff)

noout <- subset(nolowqual, mahal < cutoff)
write.csv(noout, "../03_Data/sona_data_noout.csv", row.names = F)
```

```{r sona-assumptions, eval = F}
random <- rchisq(nrow(sona_good), 7) 
fake <- lm(random ~ ., data = sona_good %>% 
             select(Q2_1:Q5_20)  %>% 
             select(-Q2_21, -Q4_20, -Q5_16))
standardized <- rstudent(fake)
fitvalues <- scale(fake$fitted.values)

# Linearity ----
{
  qqnorm(standardized)
  abline(0,1)
}

# Normality -----
hist(standardized, breaks=15)

# Homoscedasticity -----
{
  plot(fitvalues, standardized)
  abline(0,0) 
  abline(v = 0)
} 

random <- rchisq(nrow(sona_noout), 7) 
fake <- lm(random ~ ., data = sona_noout %>% 
             select(Q2_1:Q5_20)  %>% 
             select(-Q2_21, -Q4_20, -Q5_16))
standardized <- rstudent(fake)
fitvalues <- scale(fake$fitted.values)

# Linearity -----
{
  qqnorm(standardized)
  abline(0,1)
}

# Normality -----
hist(standardized, breaks=15)

# Homoscedasticity -----
{
  plot(fitvalues, standardized)
  abline(0,0) 
  abline(v = 0)
} 
```

```{r screen-mturk-accuracy-out, eval = F}
# Accuracy ----
summary(mturk %>% 
          select(Q2_1:Q2_21, Q4_1:Q4_20, Q5_1:Q5_20))
apply(mturk %>% 
          select(Q2_1:Q2_21, Q4_1:Q4_20, Q5_1:Q5_20), 
      2, mean, na.rm = TRUE)
apply(mturk %>% 
          select(Q2_1:Q2_21, Q4_1:Q4_20, Q5_1:Q5_20), 
      2, sd, na.rm = TRUE)
# everything is within the expected range

# Missing ----

# rows
missing <- apply(mturk %>% 
          select(Q2_1:Q2_21, Q4_1:Q4_20, Q5_1:Q5_20), 
          1, 
          percentmiss) 
table(missing)
replacepeople <- subset(mturk, missing <= 5)
dontpeople <- subset(mturk, missing > 5)

# columns
missingcol <- apply(replacepeople %>% 
                      select(Q2_1:Q2_21, Q4_1:Q4_20, Q5_1:Q5_20), 
                    2, 
                    percentmiss)
table(missingcol) # all replaceable

replacecolumn <- replacepeople %>% 
  select(Q2_1:Q2_21, Q4_1:Q4_20, Q5_1:Q5_20)
dontcolumn <- replacepeople %>% 
  select(-c(Q2_1:Q2_21, Q4_1:Q4_20, Q5_1:Q5_20))

# replacing those with <= 5% missing by multiple imputation
tempnomiss <- mice(replacecolumn)
nomiss <- complete(tempnomiss, 1)
summary(nomiss)

# putting data back together
# excluding participants with > 5% missing (7 total)
allcolumns <- cbind(dontcolumn, nomiss)
summary(allcolumns)
write.csv(allcolumns, "../03_Data/mturk_data_replaced.csv", row.names = F)
```

```{r screen-mturk-SAD, eval = F}
# SAD 
nomissing <- import("../03_Data/mturk_data_replaced.csv")

# Block 1
page1 <- SAD(dat = nomissing %>% 
               select(Q2_1:Q2_21),
    rt = nomissing$`Q10_Page Submit`,
    min = 1, 
    max = 5, 
    partno = nomissing$ResponseId, 
    click = nomissing$`Q10_Click Count`, 
    manvec = nomissing$Q2_21, 
    mancor = 1, 
    char = 1626)
write.csv(page1, "../03_Data/mturk_page1_sad.csv", row.names = F)

# Block 2
page2 <- SAD(dat = nomissing %>% 
               select(Q4_1:Q4_20), 
            rt = nomissing$`Q11_Page Submit`,
            min = 1, 
            max = 5, 
            partno = nomissing$ResponseId, 
            click = nomissing$`Q11_Click Count`, 
            manvec = nomissing$Q4_20, 
            mancor = 4, 
            char = 1491)
write.csv(page2, "../03_Data/mturk_page2_sad.csv", row.names = F)

# Block 3
page3 <- SAD(dat = nomissing %>% 
               select(Q5_1:Q5_20), 
            rt = nomissing$`Q12_Page Submit`,
            min = 1, 
            max = 5, 
            partno = nomissing$ResponseId, 
            click = nomissing$`Q12_Click Count`, 
            manvec = nomissing$Q5_16, 
            mancor = 3, 
            char = 1496)
write.csv(page3, "../03_Data/mturk_page3_sad.csv", row.names = F)

# Total
nomissing$totalbad <- page1$badTotal + page2$badTotal + page3$badTotal
table(nomissing$totalbad) 
sum(nomissing$totalbad < 15*.4) 
nrow(nomissing)

nomissing$totalbadUP <- 
  page1$badChar + page1$badClick + page1$badMC + 
  page2$badChar + page2$badClick + page2$badMC + 
  page3$badChar + page3$badClick + page3$badMC
table(nomissing$totalbadUP)
sum(nomissing$totalbadUP < 9*.4) 
# B&S say 2/5 which is 40%, so 40% of the 9 total is 3.6 or 4 or more

nolowqual <- subset(nomissing, totalbadUP < 4)
write.csv(nolowqual, "../03_Data/mturk_data_goodqual.csv", row.names = F)
```

```{r mturk-outliers, eval = F}
nolowqual <- import("../03_Data/mturk_data_goodqual.csv")

mahal <- mahalanobis(nolowqual %>% select(Q2_1:Q5_20),
                     colMeans(nolowqual %>% select(Q2_1:Q5_20), na.rm = TRUE),
                     cov(nolowqual %>% select(Q2_1:Q5_20), use="pairwise.complete.obs"))

cutoff <- qchisq(1 - .001,
                 ncol(nolowqual %>% select(Q2_1:Q5_20)))
ncol(nolowqual %>% select(Q2_1:Q5_20)) #df 61
cutoff #cutoff 100.8879

summary(mahal < cutoff)

noout <- subset(nolowqual, mahal < cutoff)
write.csv(noout, "../03_Data/mturk_data_noout.csv", row.names = F)
```

```{r mturk-assumptions, eval = F}
random <- rchisq(nrow(mturk_good), 7) 
fake <- lm(random ~ ., data = mturk_good %>% 
             select(Q2_1:Q5_20)  %>% 
             select(-Q2_21, -Q4_20, -Q5_16))
standardized <- rstudent(fake)
fitvalues <- scale(fake$fitted.values)

# Linearity ----
{
  qqnorm(standardized)
  abline(0,1)
}

# Normality -----
hist(standardized, breaks=15)

# Homoscedasticity -----
{
  plot(fitvalues, standardized)
  abline(0,0) 
  abline(v = 0)
} 

random <- rchisq(nrow(mturk_noout), 7) 
fake <- lm(random ~ ., data = mturk_noout %>% 
             select(Q2_1:Q5_20)  %>% 
             select(-Q2_21, -Q4_20, -Q5_16))
standardized <- rstudent(fake)
fitvalues <- scale(fake$fitted.values)

# Linearity -----
{
  qqnorm(standardized)
  abline(0,1)
}

# Normality -----
hist(standardized, breaks=15)

# Homoscedasticity -----
{
  plot(fitvalues, standardized)
  abline(0,0) 
  abline(v = 0)
} 
```

```{r data-combine-split}
combo <- bind_rows(sona_good, mturk_good)
combo_noout <- bind_rows(sona_noout, mturk_noout)

random_split <- c(
  sample(sona_good$ResponseId, nrow(sona_good)/2),
  sample(mturk_good$ResponseId, round(nrow(mturk_good)/2))
)

efaDF <- combo %>% 
  filter(ResponseId %in% random_split) %>% 
  select(Q2_1:Q5_20)  %>% 
  select(-Q2_21, -Q4_20, -Q5_16)
cfaDF <- combo %>% 
  filter(!(ResponseId %in% random_split)) %>% 
  select(Q2_1:Q5_20)  %>% 
  select(-Q2_21, -Q4_20, -Q5_16)

nrow(efaDF)
nrow(cfaDF)

efaDF_noout <- combo_noout %>% 
  filter(ResponseId %in% random_split) %>% 
  select(Q2_1:Q5_20)  %>% 
  select(-Q2_21, -Q4_20, -Q5_16)
cfaDF_noout <- combo_noout %>% 
  filter(!(ResponseId %in% random_split)) %>% 
  select(Q2_1:Q5_20)  %>% 
  select(-Q2_21, -Q4_20, -Q5_16)

nrow(efaDF_noout)
nrow(cfaDF_noout)

write.csv(efaDF, "../03_Data/efaDF.csv", row.names = F)
write.csv(cfaDF, "../03_Data/cfaDF.csv", row.names = F)
```

## EFA

### Number of Factors

```{r}
#efaDF <- read.csv("../03_Data/efaDF.csv")

number_items <- fa.parallel(efaDF, #data frame
                            fm="ml", #math
                            fa="fa") #only efa
sum(number_items$fa.values > 1)
sum(number_items$fa.values > .7)
```

### Simple Structure

```{r}
item_text <- read.csv("../03_Data/item_text.csv")

EFA_fit <- fa(efaDF, #data
              nfactors = 2, #number of factors
              rotate = "oblimin", #rotation
              fm = "ml") #math

EFA_fit

# remember you gotta fix this because of the order of operations 
efa_loadings <- as.data.frame(unclass(EFA_fit$loadings)) %>% 
  mutate(keep = ifelse(
    ML1 >= .30, "yes", ifelse(
      ML2 >= .30, "yes", ifelse(
        ML1 >= .30 & ML2 >= .30, "no", "no"
      )
    )
  )) %>% 
  mutate(question = rownames(.))

EFA_fit_2 <- fa(efaDF %>% 
                  select(efa_loadings %>% 
                           filter(keep == "yes") %>% 
                           pull(question)), #data
              nfactors = 2, #number of factors
              rotate = "oblimin", #rotation
              fm = "ml") #math

EFA_fit_2

efa_loadings_2 <- as.data.frame(unclass(EFA_fit_2$loadings)) %>% 
  mutate(keep = ifelse(
    ML1 >= .30, "yes", ifelse(
      ML2 >= .30, "yes", ifelse(
        ML1 >= .30 & ML2 >= .30, "no", "no"
      )
    )
  )) %>% 
<<<<<<< HEAD
  mutate(question = rownames(.)) %>% 
  left_join(questions,
            by = c("question" = "Question_Number")) %>% 
  arrange(desc(ML1)) %>% 
  export(., "results/2_factor.xlsx", row.names = F)
=======
  mutate(question = rownames(.))

## Make a dataframe with loadings and item text
model1_loadings <- efa_loadings_2
model1_loadings$ItemNum <- row.names(model1_loadings)
model1_withtext <- merge(item_text, model1_loadings, by = 'ItemNum')

>>>>>>> 17b96dd531fbba2d22fb53dde8aa7ddf439bf072
```

```{r}

EFA_fit <- fa(efaDF, #data
              nfactors = 3, #number of factors
              rotate = "oblimin", #rotation
              fm = "ml") #math

EFA_fit

efa_loadings <- as.data.frame(unclass(EFA_fit$loadings)) %>% 
  mutate(keep = ifelse(
    ML1 >= .30, "yes", ifelse(
      ML2 >= .30, "yes", ifelse(
        ML3 >= .30, "yes", "no"
        )
      )
    )
  ) %>% 
  mutate(question = rownames(.)) %>% 
  mutate(
    keep = ifelse(
          (ML1 >= .30 & ML2 >= .30) | 
            (ML1 >= .30 & ML3 >= .30) |
            (ML3 >= .30 & ML2 >= .30), "no", keep
          )
  )

EFA_fit_2 <- fa(efaDF %>% 
                  select(efa_loadings %>% 
                           filter(keep == "yes") %>% 
                           pull(question)), #data
              nfactors = 3, #number of factors
              rotate = "oblimin", #rotation
              fm = "ml") #math

EFA_fit_2

efa_loadings_2 <- as.data.frame(unclass(EFA_fit_2$loadings)) %>% 
  mutate(keep = ifelse(
    ML1 >= .30, "yes", ifelse(
      ML2 >= .30, "yes", ifelse(
        ML3 >= .30, "yes", "no"
        )
      )
    )
  ) %>% 
  mutate(question = rownames(.)) %>% 
  mutate(
    keep = ifelse(
          (ML1 >= .30 & ML2 >= .30) | 
            (ML1 >= .30 & ML3 >= .30) |
            (ML3 >= .30 & ML2 >= .30), "no", keep
          )
<<<<<<< HEAD
  ) %>% 
  left_join(questions,
            by = c("question" = "Question_Number")) %>% 
  arrange(desc(ML1)) %>% 
  export(., "results/3_factor.xlsx", row.names = F)
=======
  )

model2_loadings <- efa_loadings_2
model2_loadings$ItemNum <- row.names(model2_loadings)
model2_withtext <- merge(item_text, model2_loadings, by = 'ItemNum')
>>>>>>> 17b96dd531fbba2d22fb53dde8aa7ddf439bf072
```

```{r}
EFA_fit <- fa(efaDF, #data
              nfactors = 5, #number of factors
              rotate = "oblimin", #rotation
              fm = "ml") #math

EFA_fit

efa_loadings <- as.data.frame(unclass(EFA_fit$loadings)) %>% 
  mutate(question = rownames(.)) %>% 
  rowwise() %>%
  mutate(keep = sum(c_across(-question) >= .30)) %>% 
  ungroup() 

EFA_fit_2 <- fa(efaDF %>% 
                  select(efa_loadings %>% 
                           filter(keep == 1) %>% 
                           pull(question)), #data
              nfactors = 5, #number of factors
              rotate = "oblimin", #rotation
              fm = "ml") #math

EFA_fit_2

efa_loadings_2 <- as.data.frame(unclass(EFA_fit_2$loadings)) %>% 
  mutate(question = rownames(.)) %>% 
  rowwise() %>%
  mutate(keep = sum(c_across(-question) >= .30)) %>% 
  ungroup() 

EFA_fit_3 <- fa(efaDF %>% 
                  select(efa_loadings_2 %>% 
                           filter(keep == 1) %>% 
                           pull(question)), #data
              nfactors = 5, #number of factors
              rotate = "oblimin", #rotation
              fm = "ml") #math

EFA_fit_3

efa_loadings_3 <- as.data.frame(unclass(EFA_fit_3$loadings)) %>% 
  mutate(question = rownames(.)) %>% 
  rowwise() %>%
  mutate(keep = sum(c_across(-question) >= .30)) %>% 
  ungroup() %>% 
  left_join(questions,
            by = c("question" = "Question_Number")) %>% 
  arrange(desc(ML1)) %>% 
  export(., "results/5_factor.xlsx", row.names = F)

model3_loadings <- efa_loadings_3
model3_loadings$ItemNum <- model3_loadings$question
model3_withtext <- merge(item_text, model3_loadings, by = 'ItemNum')

```

-   take the final version of each "best model"
-   get down to 5 items each or whatever is left (best loading and
    theoretical)
-   look at the fit indices to tell which may be best

**Model 1: Two-Factor**

These are all the items with loading \> 0.7

-   Factor One: Sense of Contribution

    -   Q2_16: I have taken part in activities or events which will
        impact the future.

    -   Q2_2: I have contributed to the world in a unique way.

    -   Q2_3: I have contributed to causes that will impact the future.

    -   Q5_9: I have contributed to things which will be meaningful.

    -   Q5_13: I will leave a lasting legacy.

    -   Q2_5: I am proud of my accomplishments.

    -   Q2_6: I am confident in the legacy I will leave when I die.

    -   Q4_1: Others would say I have impacted their lives.

<!-- -->

-   Factor Two: Spiritual Immortality

    -   Q4_19: I believe in an afterlife.

    -   Q2_17: My spirit or soul will live on after I physically die.

    -   Q2_12: I believe I will live on in the afterlife.

    -   Q5_2: Death is not the end.

    -   Q5_10: I believe in an immaterial soul or spirit.

    -   Q5_1: I am comforted by the thought of an afterlife.

    -   Q2_18: Death will not be the end of me.

**Model 2: Three-Factor**

-   Factor One:

    -   Q2_16: I have taken part in activities or events which will
        impact the future.

    -   Q2_3: I have contributed to causes that will impact the future.

    -   Q2_5: I am proud of my accomplishments.

    -   Q5_9: I have contributed to things which will be meaningful.

    -   Q4_1: Others would say I have impacted their lives.

    -   Q2_4: I feel I have taken part in something bigger than myself.

    -   Q2_2: I have contributed to the world in a unique way.

<!-- -->

-   Factor Two:

    -   Q

<!-- -->

-   Factor Three:

    -   Q

**Model 3: Five-Factor**

# Discussion

\newpage

# References

::: {#refs custom-style="Bibliography"}
:::
